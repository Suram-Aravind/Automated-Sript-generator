{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "jD4rKXrv_KyG",
        "outputId": "c22627b0-6f70-4e0b-94d6-740ffbf27860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Pretraining ---\n",
            "Creating new tokenizer for pretraining.\n",
            "Effective vocabulary size for model layers: 8001\n",
            "Tokenizer actual word_index size: 12443\n",
            "Tokenizer saved to paragraph_tokenizer.pkl\n",
            "Initializing data generator...\n",
            "Preprocessing data to generate all samples for the generator...\n",
            "  Processing paragraph 500/4232 for sample creation\n",
            "  Processing paragraph 1000/4232 for sample creation\n",
            "  Processing paragraph 1500/4232 for sample creation\n",
            "  Processing paragraph 2000/4232 for sample creation\n",
            "  Processing paragraph 2500/4232 for sample creation\n",
            "  Processing paragraph 3000/4232 for sample creation\n",
            "  Processing paragraph 3500/4232 for sample creation\n",
            "  Processing paragraph 4000/4232 for sample creation\n",
            "DataGenerator initialized with 480349 total training samples.\n",
            "Initializing new model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting pretraining for 6 epochs using data generator...\n",
            "Epoch 1/6\n",
            "\u001b[1m7506/7506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 20ms/step - accuracy: 0.1587 - loss: 5.9311\n",
            "Epoch 2/6\n",
            "\u001b[1m7506/7506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 20ms/step - accuracy: 0.2811 - loss: 4.4765\n",
            "Epoch 3/6\n",
            "\u001b[1m7506/7506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 20ms/step - accuracy: 0.3247 - loss: 3.9062\n",
            "Epoch 4/6\n",
            "\u001b[1m7506/7506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 20ms/step - accuracy: 0.3543 - loss: 3.5471\n",
            "Epoch 5/6\n",
            "\u001b[1m7506/7506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 20ms/step - accuracy: 0.3775 - loss: 3.2947\n",
            "Epoch 6/6\n",
            "\u001b[1m7506/7506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 20ms/step - accuracy: 0.3988 - loss: 3.0852\n",
            "Pretrained model saved to paragraph_lm.keras\n",
            "--- Pretraining Finished ---\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "import math  # For ceil\n",
        "\n",
        "# --- USER: SET YOUR PRETRAINING DATA FILE PATH AND EPOCHS HERE ---\n",
        "PRETRAINING_DATA_FILE_PATH = \"/content/Physics_Dataset.txt\"  # <--- EDIT THIS\n",
        "NUM_EPOCHS_PRETRAIN = 26                                   # <--- EDIT THIS (optional)\n",
        "# --- END USER SETTINGS ---\n",
        "\n",
        "# --- Hyperparameters (Shared) ---\n",
        "VOCAB_SIZE = 8000\n",
        "EMBEDDING_DIM = 128\n",
        "LSTM_UNITS = 256\n",
        "DROPOUT_RATE = 0.2\n",
        "LEARNING_RATE = 0.001\n",
        "# BATCH_SIZE now refers to the number of sequences per training step,\n",
        "# processed by the generator.\n",
        "BATCH_SIZE = 64\n",
        "MAX_PARAGRAPH_LEN = 250\n",
        "\n",
        "# --- Fixed File Paths (Shared) ---\n",
        "MODEL_SAVE_PATH = \"paragraph_lm.keras\"\n",
        "TOKENIZER_SAVE_PATH = \"paragraph_tokenizer.pkl\"\n",
        "\n",
        "# --- Utility Functions (Shared) ---\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = re.sub(r\"[^a-z0-9\\s\\.\\,\\!\\?\\']\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def load_tokenizer(path):\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'rb') as handle:\n",
        "            tokenizer = pickle.load(handle)\n",
        "        print(f\"Tokenizer loaded from {path}\")\n",
        "        return tokenizer\n",
        "    return None\n",
        "\n",
        "def save_tokenizer(tokenizer, path):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print(f\"Tokenizer saved to {path}\")\n",
        "\n",
        "def build_model(model_vocab_size, embedding_dim, lstm_units, dropout_rate, max_len_for_embedding_layer):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(\n",
        "            input_dim=model_vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            input_length=max_len_for_embedding_layer - 1\n",
        "        ),\n",
        "        tf.keras.layers.LSTM(lstm_units),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(model_vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def load_or_initialize_model(\n",
        "    model_path,\n",
        "    model_vocab_size,\n",
        "    embedding_dim,\n",
        "    lstm_units,\n",
        "    dropout_rate,\n",
        "    max_len_for_embedding_layer,\n",
        "    learning_rate\n",
        "):\n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"Loading existing model from {model_path}\")\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(f\"Re-compiling loaded model with learning rate: {learning_rate}\")\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "    else:\n",
        "        print(\"Initializing new model.\")\n",
        "        model = build_model(\n",
        "            model_vocab_size,\n",
        "            embedding_dim,\n",
        "            lstm_units,\n",
        "            dropout_rate,\n",
        "            max_len_for_embedding_layer\n",
        "        )\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# --- DATA GENERATOR ---\n",
        "class PretrainingDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, text_data_str, tokenizer, batch_size, max_paragraph_len, model_vocab_size, shuffle=True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "        self.max_paragraph_len = max_paragraph_len  # Max length of original paragraph part to consider\n",
        "        self.model_vocab_size = model_vocab_size    # VOCAB_SIZE + 1\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.raw_paragraphs = [\n",
        "            p.strip()\n",
        "            for p in text_data_str.strip().split('\\n\\n')\n",
        "            if p.strip()\n",
        "        ]\n",
        "\n",
        "        # Create all (input_token_list, target_token_idx) pairs upfront.\n",
        "        self.samples = []\n",
        "        print(\"Preprocessing data to generate all samples for the generator...\")\n",
        "        for para_idx, raw_paragraph in enumerate(self.raw_paragraphs):\n",
        "            if (para_idx + 1) % 500 == 0:\n",
        "                print(f\"  Processing paragraph {para_idx + 1}/{len(self.raw_paragraphs)} for sample creation\")\n",
        "\n",
        "            cleaned_paragraph = clean_text(raw_paragraph)\n",
        "            token_list = self.tokenizer.texts_to_sequences([cleaned_paragraph])[0]\n",
        "\n",
        "            if not token_list or len(token_list) < 2:\n",
        "                continue\n",
        "\n",
        "            # Truncate paragraph tokens if longer than max_paragraph_len\n",
        "            token_list = token_list[:self.max_paragraph_len]\n",
        "\n",
        "            for i in range(1, len(token_list)):\n",
        "                input_seq_tokens = token_list[:i]  # Input is up to word i-1\n",
        "                target_token_id = token_list[i]    # Target is word i\n",
        "\n",
        "                # Ensure target_token_id is valid\n",
        "                if target_token_id < self.model_vocab_size:\n",
        "                    self.samples.append((input_seq_tokens, target_token_id))\n",
        "\n",
        "        print(f\"DataGenerator initialized with {len(self.samples)} total training samples.\")\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of batches per epoch\n",
        "        return math.ceil(len(self.samples) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, batch_idx):\n",
        "        batch_start_idx = batch_idx * self.batch_size\n",
        "        batch_end_idx = (batch_idx + 1) * self.batch_size\n",
        "        batch_samples = self.samples[batch_start_idx:batch_end_idx]\n",
        "\n",
        "        batch_input_token_lists = [sample[0] for sample in batch_samples]\n",
        "        batch_target_token_ids = [sample[1] for sample in batch_samples]\n",
        "\n",
        "        # Pad input sequences for this batch\n",
        "        padded_input_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            batch_input_token_lists,\n",
        "            maxlen=self.max_paragraph_len - 1,\n",
        "            padding='pre',\n",
        "            truncating='pre'\n",
        "        )\n",
        "\n",
        "        # One-hot encode target words for this batch\n",
        "        categorical_target_words = tf.keras.utils.to_categorical(\n",
        "            batch_target_token_ids,\n",
        "            num_classes=self.model_vocab_size\n",
        "        )\n",
        "\n",
        "        return padded_input_sequences, categorical_target_words\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle samples at the end of each epoch if shuffle is True\n",
        "        if self.shuffle:\n",
        "            indices = np.arange(len(self.samples))\n",
        "            np.random.shuffle(indices)\n",
        "            self.samples = [self.samples[i] for i in indices]\n",
        "\n",
        "# --- END DATA GENERATOR ---\n",
        "\n",
        "def run_pretraining():\n",
        "    print(\"\\n--- Starting Pretraining ---\")\n",
        "\n",
        "    # Read the entire text data for tokenizer fitting and generator sample creation\n",
        "    with open(PRETRAINING_DATA_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "        text_data = f.read()\n",
        "\n",
        "    # --- Tokenizer Initialization and Fitting ---\n",
        "    paragraphs_for_tokenizer = [\n",
        "        clean_text(p)\n",
        "        for p in text_data.strip().split('\\n\\n')\n",
        "        if p.strip()\n",
        "    ]\n",
        "    tokenizer = load_tokenizer(TOKENIZER_SAVE_PATH)\n",
        "    if tokenizer:\n",
        "        print(\"Updating existing tokenizer with new pretraining data.\")\n",
        "        if tokenizer.num_words != VOCAB_SIZE:\n",
        "            print(\n",
        "                f\"Warning: Loaded tokenizer num_words ({tokenizer.num_words}) \"\n",
        "                f\"!= VOCAB_SIZE ({VOCAB_SIZE}). Re-initializing.\"\n",
        "            )\n",
        "            tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<unk>\")\n",
        "        tokenizer.fit_on_texts(paragraphs_for_tokenizer)\n",
        "    else:\n",
        "        print(\"Creating new tokenizer for pretraining.\")\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<unk>\")\n",
        "        tokenizer.fit_on_texts(paragraphs_for_tokenizer)\n",
        "\n",
        "    model_vocab_size = VOCAB_SIZE + 1\n",
        "    print(f\"Effective vocabulary size for model layers: {model_vocab_size}\")\n",
        "    print(f\"Tokenizer actual word_index size: {len(tokenizer.word_index)}\")\n",
        "    save_tokenizer(tokenizer, TOKENIZER_SAVE_PATH)\n",
        "\n",
        "    # --- Initialize Data Generator ---\n",
        "    print(\"Initializing data generator...\")\n",
        "    pretrain_generator = PretrainingDataGenerator(\n",
        "        text_data_str=text_data,\n",
        "        tokenizer=tokenizer,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_paragraph_len=MAX_PARAGRAPH_LEN,\n",
        "        model_vocab_size=model_vocab_size\n",
        "    )\n",
        "\n",
        "    if len(pretrain_generator.samples) == 0:\n",
        "        print(\"No training samples were generated. Check your data or preprocessing. Aborting.\")\n",
        "        return\n",
        "\n",
        "    # --- Load or Initialize Model ---\n",
        "    model = load_or_initialize_model(\n",
        "        MODEL_SAVE_PATH,\n",
        "        model_vocab_size,\n",
        "        EMBEDDING_DIM,\n",
        "        LSTM_UNITS,\n",
        "        DROPOUT_RATE,\n",
        "        MAX_PARAGRAPH_LEN,\n",
        "        LEARNING_RATE\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # --- Train the Model using the Generator ---\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    print(f\"\\nStarting pretraining for {NUM_EPOCHS_PRETRAIN} epochs using data generator...\")\n",
        "    model.fit(\n",
        "        pretrain_generator,\n",
        "        epochs=NUM_EPOCHS_PRETRAIN,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    model.save(MODEL_SAVE_PATH)\n",
        "    print(f\"Pretrained model saved to {MODEL_SAVE_PATH}\")\n",
        "    print(\"--- Pretraining Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pretraining()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HWWG-5on_VYP",
        "outputId": "247aa953-93bc-4ea8-a866-dfefbebe5feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mixed precision enabled\n",
            "\n",
            "--- Starting Supervised Fine-Tuning ---\n",
            "Tokenizer loaded from paragraph_tokenizer.pkl\n",
            "Using effective vocabulary size for SFT model layers: 8001\n",
            "Found 4232 SFT pairs for preprocessing.\n",
            "Processing SFT pair 100/4232\n",
            "Processing SFT pair 200/4232\n",
            "Processing SFT pair 300/4232\n",
            "Processing SFT pair 400/4232\n",
            "Processing SFT pair 500/4232\n",
            "Processing SFT pair 600/4232\n",
            "Processing SFT pair 700/4232\n",
            "Processing SFT pair 800/4232\n",
            "Processing SFT pair 900/4232\n",
            "Processing SFT pair 1000/4232\n",
            "Processing SFT pair 1100/4232\n",
            "Processing SFT pair 1200/4232\n",
            "Processing SFT pair 1300/4232\n",
            "Processing SFT pair 1400/4232\n",
            "Processing SFT pair 1500/4232\n",
            "Processing SFT pair 1600/4232\n",
            "Processing SFT pair 1700/4232\n",
            "Processing SFT pair 1800/4232\n",
            "Processing SFT pair 1900/4232\n",
            "Processing SFT pair 2000/4232\n",
            "Processing SFT pair 2100/4232\n",
            "Processing SFT pair 2200/4232\n",
            "Processing SFT pair 2300/4232\n",
            "Processing SFT pair 2400/4232\n",
            "Processing SFT pair 2500/4232\n",
            "Processing SFT pair 2600/4232\n",
            "Processing SFT pair 2700/4232\n",
            "Processing SFT pair 2800/4232\n",
            "Processing SFT pair 2900/4232\n",
            "Processing SFT pair 3000/4232\n",
            "Processing SFT pair 3100/4232\n",
            "Processing SFT pair 3200/4232\n",
            "Processing SFT pair 3300/4232\n",
            "Processing SFT pair 3400/4232\n",
            "Processing SFT pair 3500/4232\n",
            "Processing SFT pair 3600/4232\n",
            "Processing SFT pair 3700/4232\n",
            "Processing SFT pair 3800/4232\n",
            "Processing SFT pair 3900/4232\n",
            "Processing SFT pair 4000/4232\n",
            "Processing SFT pair 4100/4232\n",
            "Processing SFT pair 4200/4232\n",
            "Generated 480349 SFT training sequences.\n",
            "Loading existing model for SFT from paragraph_lm.keras\n",
            "Re-compiling loaded model for SFT with learning rate: 0.0001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m149\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,024,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m394,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8001\u001b[0m)           │     \u001b[38;5;34m2,056,257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8001</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056,257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,474,625\u001b[0m (13.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,474,625</span> (13.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,474,625\u001b[0m (13.25 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,474,625</span> (13.25 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "import json # For SFT data\n",
        "# import tensorflow as tf\n",
        "# ... other imports ...\n",
        "\n",
        "# Enable mixed precision for GPU training (if available)\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n",
        "print('Mixed precision enabled')\n",
        "\n",
        "# --- USER: SET YOUR SFT DATA FILE PATH AND EPOCHS HERE ---\n",
        "SFT_DATA_FILE_PATH = \"/content/new.json\"  # <--- EDIT THIS (JSONL format: {\"prompt\": \"...\", \"response\": \"...\"})\n",
        "NUM_EPOCHS_SFT = 5                          # <--- EDIT THIS (optional)\n",
        "# --- END USER SETTINGS ---\n",
        "\n",
        "# --- Hyperparameters (Shared) ---\n",
        "VOCAB_SIZE = 8000        # Must match pretraining. Determines model layer sizes.\n",
        "EMBEDDING_DIM = 128\n",
        "LSTM_UNITS = 256\n",
        "DROPOUT_RATE = 0.2\n",
        "SFT_LEARNING_RATE = 0.0001 # Typically smaller for fine-tuning\n",
        "BATCH_SIZE = 128\n",
        "MAX_PARAGRAPH_LEN = 250  # Must match pretraining.\n",
        "\n",
        "# --- Fixed File Paths (Shared) ---\n",
        "MODEL_SAVE_PATH = \"paragraph_lm.keras\"\n",
        "TOKENIZER_SAVE_PATH = \"paragraph_tokenizer.pkl\"\n",
        "\n",
        "# --- Utility Functions (Shared - Copied for script independence) ---\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = re.sub(r\"[^a-z0-9\\s\\.\\,\\!\\?\\']\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def load_tokenizer(path):\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'rb') as handle:\n",
        "            tokenizer = pickle.load(handle)\n",
        "        print(f\"Tokenizer loaded from {path}\")\n",
        "        return tokenizer\n",
        "    print(f\"ERROR: Tokenizer not found at {path}. Pretrain first.\")\n",
        "    return None\n",
        "\n",
        "def save_tokenizer(tokenizer, path): # Might be used if tokenizer is updated during SFT\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print(f\"Tokenizer saved to {path}\")\n",
        "\n",
        "def build_model(model_vocab_size, embedding_dim, lstm_units, dropout_rate, max_len_for_embedding_layer):\n",
        "    # This function is mainly for initial creation, SFT loads an existing model.\n",
        "    # However, keeping it for consistency in case load_or_initialize_model needs it.\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=model_vocab_size,\n",
        "                                  output_dim=embedding_dim,\n",
        "                                  input_length=max_len_for_embedding_layer-1),\n",
        "        tf.keras.layers.LSTM(lstm_units),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(model_vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_sft_data(sft_pairs, tokenizer, max_len, model_vocab_size_for_output_layer):\n",
        "    input_sequences = []\n",
        "    target_words = []\n",
        "    print(f\"Found {len(sft_pairs)} SFT pairs for preprocessing.\")\n",
        "\n",
        "    for i, (prompt, response) in enumerate(sft_pairs):\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Processing SFT pair {i + 1}/{len(sft_pairs)}\")\n",
        "\n",
        "        full_text = clean_text(prompt + \" \" + response) # Combine prompt and response\n",
        "        token_list = tokenizer.texts_to_sequences([full_text])[0]\n",
        "\n",
        "        if not token_list or len(token_list) < 2:\n",
        "            continue\n",
        "\n",
        "        token_list = token_list[:max_len] # Truncate\n",
        "\n",
        "        for j in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:j+1]\n",
        "            input_seq = n_gram_sequence[:-1]\n",
        "            target_idx = n_gram_sequence[-1]\n",
        "\n",
        "            if target_idx < model_vocab_size_for_output_layer:\n",
        "                input_sequences.append(input_seq)\n",
        "                target_words.append(target_idx)\n",
        "\n",
        "    if not input_sequences:\n",
        "        print(\"No valid SFT sequences generated.\")\n",
        "        return None, None\n",
        "\n",
        "    padded_input_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        input_sequences, maxlen=max_len-1, padding='pre'\n",
        "    )\n",
        "    categorical_target_words = tf.keras.utils.to_categorical(\n",
        "        target_words, num_classes=model_vocab_size_for_output_layer\n",
        "    )\n",
        "    print(f\"Generated {len(padded_input_sequences)} SFT training sequences.\")\n",
        "    return padded_input_sequences, categorical_target_words\n",
        "# --- End Utility Functions ---\n",
        "\n",
        "def run_sft():\n",
        "    print(\"\\n--- Starting Supervised Fine-Tuning ---\")\n",
        "\n",
        "    if SFT_DATA_FILE_PATH == \"your_sft_data.jsonl\":\n",
        "        print(\"ERROR: Please edit 'SFT_DATA_FILE_PATH' in the script with your actual data file path.\")\n",
        "        return\n",
        "\n",
        "    sft_pairs = []\n",
        "    try:\n",
        "        with open(SFT_DATA_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "            for line_num, line in enumerate(f):\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    sft_pairs.append((data['prompt'], data['response']))\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Warning: SFT data, line {line_num+1}: Could not parse JSON: {line.strip()}\")\n",
        "                except KeyError:\n",
        "                    print(f\"Warning: SFT data, line {line_num+1}: Missing 'prompt' or 'response': {line.strip()}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: SFT data file not found at '{SFT_DATA_FILE_PATH}'\")\n",
        "        return\n",
        "\n",
        "    if not sft_pairs:\n",
        "        print(\"No SFT data loaded. Aborting SFT.\")\n",
        "        return\n",
        "\n",
        "    tokenizer = load_tokenizer(TOKENIZER_SAVE_PATH)\n",
        "    if not tokenizer:\n",
        "        return # Error message already printed by load_tokenizer\n",
        "\n",
        "    # Important: VOCAB_SIZE hyperparameter must be consistent with the pretraining phase\n",
        "    # as it determines the model's layer dimensions.\n",
        "    # The tokenizer.num_words should reflect this.\n",
        "    if tokenizer.num_words != VOCAB_SIZE:\n",
        "        print(f\"CRITICAL WARNING: Loaded tokenizer has num_words={tokenizer.num_words}, \"\n",
        "              f\"but SFT script's VOCAB_SIZE is {VOCAB_SIZE}. These must match the settings \"\n",
        "              f\"used during pretraining when the model was created/last saved. Aborting.\")\n",
        "        return\n",
        "\n",
        "    model_vocab_size = VOCAB_SIZE + 1 # Consistent with pretraining\n",
        "    print(f\"Using effective vocabulary size for SFT model layers: {model_vocab_size}\")\n",
        "\n",
        "    # Note: We are NOT refitting the tokenizer on SFT data here by default.\n",
        "    # If SFT data has many new words critical for learning, consider:\n",
        "    # 1. Adding SFT text to pretraining data and re-pretraining.\n",
        "    # 2. Carefully updating tokenizer and potentially resizing model embedding/output layers (complex).\n",
        "    # For this setup, new words in SFT data will be treated as <unk> if not in pre-trained tokenizer.\n",
        "\n",
        "    input_seqs, target_words_cat = preprocess_sft_data(\n",
        "        sft_pairs, tokenizer, MAX_PARAGRAPH_LEN, model_vocab_size\n",
        "    )\n",
        "\n",
        "    if input_seqs is None:\n",
        "        print(\"SFT aborted: No sequences generated.\")\n",
        "        return\n",
        "\n",
        "    # Load the pre-trained model. It MUST exist for SFT.\n",
        "    model = load_or_initialize_model(\n",
        "        MODEL_SAVE_PATH, model_vocab_size, EMBEDDING_DIM, LSTM_UNITS,\n",
        "        DROPOUT_RATE, MAX_PARAGRAPH_LEN, SFT_LEARNING_RATE # Use SFT_LEARNING_RATE\n",
        "    )\n",
        "\n",
        "    if model is None: # load_or_initialize_model returns None on critical errors\n",
        "        print(\"SFT aborted due to model loading issues.\")\n",
        "        return\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_seqs, target_words_cat))\n",
        "    dataset = dataset.shuffle(buffer_size=len(input_seqs)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    print(f\"\\nStarting fine-tuning for {NUM_EPOCHS_SFT} epochs...\")\n",
        "    model.fit(dataset, epochs=NUM_EPOCHS_SFT, callbacks=[early_stopping])\n",
        "\n",
        "    model.save(MODEL_SAVE_PATH) # Overwrites the model with the fine-tuned one\n",
        "    print(f\"Fine-tuned model saved to {MODEL_SAVE_PATH}\")\n",
        "    print(\"--- Supervised Fine-Tuning Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_sft()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYjB3dLq_Y9b",
        "outputId": "d44c459e-29c6-4a4c-b5d0-8a21e330d930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Interactive Inference Mode ---\n",
            "Tokenizer loaded from paragraph_tokenizer.pkl\n",
            "Model loaded successfully.\n",
            "\n",
            "Enter 'quit' to exit inference mode.\n",
            "Enter your prompt: Energy is the capacity to do work\n",
            "Enter temperature (e.g., 0.7, default 1.0, 0 for greedy): 0.5\n",
            "Max new words to generate (e.g., 50, default 50): 25\n",
            "\n",
            "Generating text from seed: 'Energy is the capacity to do work' with temp 0.5, max_len 25\n",
            "\n",
            "Model Response:\n",
            "Energy is the capacity to do work is a fundamental property of waves and is governed by the maximum energy of the light but it is expressed as p t where p\n",
            "\n",
            "Enter your prompt: quit\n",
            "--- Exiting Inference Mode ---\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "\n",
        "# --- Hyperparameters (Must match model training) ---\n",
        "VOCAB_SIZE = 8000        # Must match the VOCAB_SIZE used for training the loaded model\n",
        "MAX_PARAGRAPH_LEN = 250  # Must match the MAX_PARAGRAPH_LEN used for training\n",
        "\n",
        "# --- Fixed File Paths (Shared) ---\n",
        "MODEL_SAVE_PATH = \"paragraph_lm.keras\"\n",
        "TOKENIZER_SAVE_PATH = \"paragraph_tokenizer.pkl\"\n",
        "\n",
        "# --- Utility Functions (Shared - Copied for script independence) ---\n",
        "def clean_text(text): # Simplified for inference input3\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def load_tokenizer(path):\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'rb') as handle:\n",
        "            tokenizer = pickle.load(handle)\n",
        "        print(f\"Tokenizer loaded from {path}\")\n",
        "        return tokenizer\n",
        "    print(f\"ERROR: Tokenizer not found at {path}. Train a model first.\")\n",
        "    return None\n",
        "\n",
        "def sample_from_probs(probs, temp):\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    if temp <= 0 or temp == float('inf'): # temp=0 greedy, temp=inf uniform random (approx)\n",
        "        return np.argmax(probs)\n",
        "\n",
        "    # Add a small epsilon to prevent log(0) and ensure sum is not zero after division\n",
        "    probs = np.log(probs + 1e-9) / temp\n",
        "    exp_preds = np.exp(probs)\n",
        "    preds = exp_preds / (np.sum(exp_preds) + 1e-9) # Add epsilon to denominator too\n",
        "\n",
        "    try:\n",
        "        # Ensure probabilities sum to 1 for multinomial\n",
        "        preds = preds / np.sum(preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        return np.argmax(probas)\n",
        "    except ValueError as e:\n",
        "        # print(f\"Warning: Multinomial sampling failed ('{e}'). Sum of preds: {np.sum(preds)}. Falling back to argmax.\")\n",
        "        return np.argmax(preds) # Fallback to greedy if sampling fails\n",
        "# --- End Utility Functions ---\n",
        "\n",
        "def generate_text_for_inference(model, tokenizer, seed_text, max_generated_len=50, temperature=1.0):\n",
        "    print(f\"\\nGenerating text from seed: '{seed_text}' with temp {temperature}, max_len {max_generated_len}\")\n",
        "\n",
        "    current_text_for_model = clean_text(seed_text) # Model expects cleaned input\n",
        "    generated_suffix = \"\" # Store only newly generated words to append to original seed\n",
        "\n",
        "    # The model's vocabulary size, including padding (index 0) and OOV\n",
        "    # This should match the output dimension of the model's dense layer.\n",
        "    model_vocab_size = VOCAB_SIZE + 1\n",
        "\n",
        "    for _ in range(max_generated_len):\n",
        "        token_list = tokenizer.texts_to_sequences([current_text_for_model])[0]\n",
        "\n",
        "        if not token_list:\n",
        "            # print(\"Warning: Token list became empty during generation.\")\n",
        "            break\n",
        "\n",
        "        # Pad the sequence to the model's expected input length\n",
        "        padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [token_list], maxlen=MAX_PARAGRAPH_LEN-1, padding='pre' # MAX_PARAGRAPH_LEN-1 for model input\n",
        "        )\n",
        "\n",
        "        predicted_probs = model.predict(padded_sequence, verbose=0)[0]\n",
        "\n",
        "        # Ensure predicted_probs length matches model_vocab_size.\n",
        "        # This might not be strictly necessary if model construction is always correct, but good sanity check.\n",
        "        if len(predicted_probs) != model_vocab_size:\n",
        "            print(f\"Warning: Predicted_probs length ({len(predicted_probs)}) \"\n",
        "                  f\"mismatches model_vocab_size ({model_vocab_size}).\")\n",
        "            # Potentially pad or truncate predicted_probs, or error out.\n",
        "            # For now, we assume it matches.\n",
        "\n",
        "        predicted_word_index = sample_from_probs(predicted_probs, temperature)\n",
        "\n",
        "        # Stop if padding (index 0) is predicted\n",
        "        if predicted_word_index == 0:\n",
        "            # print(\"Prediction stopped: Padding token predicted.\")\n",
        "            break\n",
        "\n",
        "        output_word = tokenizer.index_word.get(predicted_word_index)\n",
        "\n",
        "        # Stop if OOV token is predicted or word not found (shouldn't happen with valid index)\n",
        "        if not output_word or output_word == tokenizer.oov_token:\n",
        "            # print(f\"Prediction stopped: OOV ('{tokenizer.oov_token}') or no word for index {predicted_word_index}.\")\n",
        "            break\n",
        "\n",
        "        generated_suffix += \" \" + output_word\n",
        "        current_text_for_model += \" \" + output_word # Append to the input for the next prediction step\n",
        "\n",
        "        # Optional: Stop on sentence-ending punctuation\n",
        "        if output_word in ['.', '!', '?'] and len(generated_suffix.strip()) > 0 : # Check len to avoid stopping on first word if it's punctuation\n",
        "            # print(\"Sentence end punctuation detected.\")\n",
        "            break\n",
        "\n",
        "    return seed_text + generated_suffix # Append generated part to original, uncleaned seed\n",
        "\n",
        "\n",
        "def run_interactive_inference():\n",
        "    print(\"\\n--- Starting Interactive Inference Mode ---\")\n",
        "\n",
        "    tokenizer = load_tokenizer(TOKENIZER_SAVE_PATH)\n",
        "    if not tokenizer:\n",
        "        return\n",
        "\n",
        "    # Verify tokenizer's num_words matches VOCAB_SIZE hyperparameter. Critical for consistency.\n",
        "    if tokenizer.num_words != VOCAB_SIZE:\n",
        "        print(f\"CRITICAL WARNING: Loaded tokenizer has num_words={tokenizer.num_words}, \"\n",
        "              f\"but Inference script's VOCAB_SIZE is {VOCAB_SIZE}. These must match the settings \"\n",
        "              f\"used during model training. Inference might be incorrect.\")\n",
        "        # Decide whether to proceed or abort. For now, we proceed with a warning.\n",
        "\n",
        "    if not os.path.exists(MODEL_SAVE_PATH):\n",
        "        print(f\"Error: Model not found at {MODEL_SAVE_PATH}. Train a model first.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
        "        print(\"Model loaded successfully.\")\n",
        "        # model.summary() # Optional: print summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nEnter 'quit' to exit inference mode.\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_prompt = input(\"Enter your prompt: \")\n",
        "            if user_prompt.lower() == 'quit':\n",
        "                break\n",
        "            if not user_prompt.strip():\n",
        "                print(\"Prompt cannot be empty.\")\n",
        "                continue\n",
        "\n",
        "            temp_str = input(f\"Enter temperature (e.g., 0.7, default 1.0, 0 for greedy): \")\n",
        "            try:\n",
        "                temperature = float(temp_str) if temp_str.strip() else 1.0\n",
        "            except ValueError:\n",
        "                print(\"Invalid temperature, using 1.0.\")\n",
        "                temperature = 1.0\n",
        "\n",
        "            max_len_str = input(f\"Max new words to generate (e.g., 50, default 50): \")\n",
        "            try:\n",
        "                max_gen_len = int(max_len_str) if max_len_str.strip() else 50\n",
        "                if max_gen_len <=0: max_gen_len = 50\n",
        "            except ValueError:\n",
        "                print(\"Invalid max length, using 50.\")\n",
        "                max_gen_len = 50\n",
        "\n",
        "            response = generate_text_for_inference(model, tokenizer, user_prompt,\n",
        "                                                   max_generated_len=max_gen_len,\n",
        "                                                   temperature=temperature)\n",
        "            print(f\"\\nModel Response:\\n{response}\\n\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nInference interrupted by user.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during inference: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    print(\"--- Exiting Inference Mode ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_interactive_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC-OKmSmhYwg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "2d10db0b-03f5-4d2b-ce94-e2f22cc73a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for as\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'st' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7135e49992fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKENIZER_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"paragraph_lm.keras\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"paragraph_tokenizer.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_resource\u001b[0m \u001b[0;31m# Load model & tokenizer once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
          ]
        }
      ],
      "source": [
        "!pip install streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle, re\n",
        "\n",
        "# --- 1. USER: VERIFY THESE CONSTANTS MATCH YOUR TRAINING SCRIPT ---\n",
        "VOCAB_SIZE, MAX_PARAGRAPH_LEN = 8000, 250 # <<< EDIT if different from training\n",
        "MODEL_INPUT_LEN = MAX_PARAGRAPH_LEN - 1   # Model expects input_length = MAX_PARAGRAPH_LEN - 1\n",
        "MODEL_PATH, TOKENIZER_PATH = \"paragraph_lm.keras\", \"paragraph_tokenizer.pkl\"\n",
        "\n",
        "@st.cache_resource # Load model & tokenizer once\n",
        "def load_resources():\n",
        "    try:\n",
        "        tok = pickle.load(open(TOKENIZER_PATH, 'rb'))\n",
        "        mod = tf.keras.models.load_model(MODEL_PATH)\n",
        "        return mod, tok\n",
        "    except Exception as e:\n",
        "        st.error(f\"🔴 Error loading files: {e}. Ensure '{MODEL_PATH}' & '{TOKENIZER_PATH}' are present.\")\n",
        "        return None, None\n",
        "model, tokenizer = load_resources()\n",
        "\n",
        "st.set_page_config(page_title=\"Nano LLM\", layout=\"centered\") # <<< EDIT Page Tab Title\n",
        "st.title(\"✍️ Nano Script Spark\") # <<< EDIT Main Title\n",
        "\n",
        "if model and tokenizer:\n",
        "    prompt = st.text_area(\"🎬 Your Script Idea:\", \"In a world filled with magic...\", height=80, placeholder=\"Start typing your script idea here...\") # <<< EDIT Prompt Label & Placeholder\n",
        "\n",
        "    col1, col2 = st.columns([0.6, 0.4]) # Adjust column ratios if needed\n",
        "    with col1: temp = st.slider(\"🌡️ Creativity (Temp)\", 0.0, 2.0, 0.7, 0.05, help=\"0: more predictable, >1: more random\") # <<< EDIT Slider Label\n",
        "    with col2: max_new = st.number_input(\"📝 Max New Words\", 10, MODEL_INPUT_LEN - 10, 50, 5) # <<< EDIT Number Input Label\n",
        "\n",
        "    if st.button(\"✨ Generate Script!\", type=\"primary\", use_container_width=True): # <<< EDIT Button Text\n",
        "        if prompt.strip():\n",
        "            with st.spinner(\"⏳ AI is crafting...\"): # <<< EDIT Spinner Text\n",
        "                try:\n",
        "                    current_text = re.sub(r\"\\s+\", \" \", prompt.lower().strip()) # Basic cleaning\n",
        "                    gen_suffix = \"\"\n",
        "                    for _ in range(max_new):\n",
        "                        tokens = tokenizer.texts_to_sequences([current_text])[0]\n",
        "                        if not tokens: break\n",
        "                        padded = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=MODEL_INPUT_LEN, padding='pre')\n",
        "                        probs = model.predict(padded, verbose=0)[0].astype('float64')\n",
        "\n",
        "                        if temp <= 0: idx = np.argmax(probs) # Greedy\n",
        "                        else: # Sampling with temperature\n",
        "                            log_probs = np.log(probs + 1e-9) / temp; exp_probs = np.exp(log_probs)\n",
        "                            norm_probs = exp_probs / (np.sum(exp_probs) + 1e-9)\n",
        "                            try: idx = np.argmax(np.random.multinomial(1, norm_probs / np.sum(norm_probs), 1))\n",
        "                            except: idx = np.argmax(norm_probs) # Fallback\n",
        "\n",
        "                        if idx == 0: break # Predicted padding token\n",
        "                        word = tokenizer.index_word.get(idx)\n",
        "                        if not word or word == tokenizer.oov_token: break # Predicted OOV or unknown\n",
        "\n",
        "                        gen_suffix += \" \" + word\n",
        "                        current_text += \" \" + word\n",
        "                        if word in ['.', '!', '?'] and len(gen_suffix.strip()) > 1: break # Stop on sentence end\n",
        "\n",
        "                    st.subheader(\"📜 Generated Snippet:\") # <<< EDIT Output Header\n",
        "                    st.markdown(f\"> {prompt.strip()}{gen_suffix.strip()}\") # Display with quote style\n",
        "                except Exception as e: st.error(f\"⚠️ Generation error: {e}\")\n",
        "        else: st.warning(\"💡 Please enter a script idea to start.\") # <<< EDIT Empty Prompt Warning\n",
        "else:\n",
        "    st.error(\"🔴 Critical: Model or Tokenizer could not be loaded. Check file paths and console.\")\n",
        "\n",
        "# To run: streamlit run app.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVhEADmIeu04"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}